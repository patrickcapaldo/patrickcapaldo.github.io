---
layout: post
title: The 90/90 Rule of AI Industrialisation
description: The first 90% of the AI system takes 90% of the development time and the remaining 10% of the AI system takes the remaining 90% of the development time.
tags: artificial-intelligence
categories: blogs
date: 2025-06-28
featured: false
thumbnail: assets/img/blogs/indus_90_90_rule/iceberg.png
authors:
  - name: Patrick Capaldo
    url: "https://patrickcapaldo.github.io/"
    affiliations:
      name: None

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

This post was originally made on my Substack, Hidden Layers. Check it out [here.](https://patrickcapaldo.substack.com/p/the-9090-rule-of-ai-industrialisation)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/iceberg.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The prototype is the tip of the iceberg. Source: Unknown.
</div>

## Before You Read

**Who Is This For:** Technology strategists, engineering leaders, ML/platform teams, product managers and decision-makers responsible for transforming AI prototypes into reliable and scalable systems.

**What You’ll Learn:**
- How Zillow’s Zestimate mistake exhibits the dangers of neglecting the complexity of production-grade systems.
- The five domains that often consume more effort that the prototype itself: data governance, observability, infrastructure pipelines, security/compliance, and maintenance/versioning.
- Why Shell’s predictive maintenance AI succeeded by ingraining scalability, observability and resilience.

## Zillow’s Zestimate Breakdown

For over a decade, Zillow established itself as a dominant online real estate marketplace. Its primary business model was straightforward and highly profitable: it acted as a media company, attracting millions of prospective home buyers to its website and app with property listings and then selling advertising and lead-generation services to real estate agents who wanted to connect with that audience [1]. Central to this strategy was the "Zestimate," a proprietary AI-powered home valuation tool. For years, the Zestimate served as a popular, informational feature - a conversation starter that drew immense traffic to the platform, which Zillow then monetised through its "Premier Agent" advertising program. It was never intended to be an instrument for actual transactions.

That all changed in 2018. In a bold strategic pivot, Zillow decided to transform itself from a media company into a direct market participant. They launched "Zillow Offers," an "iBuyer" (instant buyer) service designed to leverage the Zestimate algorithm for a new purpose: making instant, all-cash offers on homes directly to sellers. The plan was to become a "market maker." For a consumer, the value was clear: they could skip the lengthy, uncertain process of listing their home and receive a fair cash offer with a guaranteed closing date. For Zillow, the plan was to charge a service fee comparable to an agent's commission, perform minor renovations, and then resell the home on the open market for a profit a few months later [2]. This new model staked the company's future on the accuracy of its AI.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/zillow_offers.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The user interface for Zillow Offers. Source: GeekWire (2021).
</div>

At first, in a stable and rising housing market, the model appeared to work. The prototype’s accuracy, within 2–3% of the eventual sale price, looked impressive. But as market conditions shifted dramatically in mid-2021, with unprecedented housing price volatility and soaring labour and material costs, the model's performance degraded. The algorithm, trained on historical data, failed to predict the market's sudden cooling and began to aggressively overbid on properties. Zillow was now algorithmically committed to buying thousands of homes for prices higher than what they would be worth by the time they could be resold. This critical failure led to a staggering US$304 million inventory write-down and forced the company to shutter the entire Zillow Offers programme, laying off 25 percent of its staff [3].

We can watch this story unfold by observing the share price of Zillow from pre-2018 to post-2022 [4-11]. Before early 2021, the share price soars with the housing boom as “work-from-home” increases demand and after-effects of Covid reduce supply, resulting in more competition, more transactions, and a stronger business. However, it all unravelled as quickly as it manifested once the failure of Zillow Offers was realised.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/zillow_share.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The rapid rise and decline of Zillow's share price (screenshot taken on 26 June 2025). Source: Google.
</div>

So what went so wrong? While the strategy itself was high-risk, the operational execution sealed its fate. The issues were not in the model's initial concept but in its industrial-scale fragility. The company lacked adequate real-time performance monitoring to detect the model's drift as market dynamics changed. No automated alerts triggered when error rates began to spike. The system was built without a robust fallback strategy, so when the AI's valuations proved unreliable, the entire automated purchasing engine began accumulating massive losses. Because the system was never fully integrated into a resilient production pipeline (complete with continuous evaluation, scale testing, and audit logging) the team was left scrambling to diagnose the problem under intense public scrutiny. As CEO Rich Barton later conceded, “Our observed error rate has been far more volatile than we ever expected possible” [3]. Zillow’s collapse is the definitive lesson that a prototype's success in the lab means nothing without the industrial-grade systems to support it in the wild.

## The “90/90 Rule” Explained

Software engineers have long shared a piece of cynical wisdom that captures the hidden complexities of their craft:

“The first 90 percent of the code accounts for the first 90 percent of the development time; the remaining 10 percent accounts for the other 90 percent.” [12]

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/indus_90_90.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The 90/90 rule. Source: t2informatik.
</div>

This “90/90 Rule,” first attributed to Tom Cargill at Bell Labs, perfectly describes the severe, but often hidden, disconnect between a clever prototype and a finished product. In AI projects, bringing your prototype into the real world is known industrialisation, the comprehensive work required to turn research code into a robust, maintainable, and scalable service. It is the silent majority of the work, encompassing data governance, observability, deployment pipelines, security controls, and maintenance processes. Each of these domains surfaces hidden dependencies and edge cases that remain invisible until real users and live workloads begin to exercise the system.

## The Industrialisation Gap

Concrete data reveals that a significant portion of AI projects stall after the proof-of-concept stage. Gartner forecasts that 30 percent of generative AI initiatives will be abandoned after their initial trials by the end of 2025, citing predictable culprits like “spiralling infra costs,” “data quality gaps,” and “insufficient risk governance” [13].

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/indus_gap.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The Industrialisation Gap. Source: Gemini (AI generated).
</div>

This trend appears to be worsening. S&P Global Market Intelligence found that a startling 42 percent of organisations scrapped most of their AI prototypes this year which is a dramatic increase from 17 percent last year. Teams often blame this on inadequate data pipelines and runaway eleventh-hour engineering work [14].

This "pilot paradox" is further confirmed by another Gartner survey, which reports that only 54 percent of AI models successfully progress to sustained production, a figure that has barely changed over three years despite soaring investment in the field [15]. These metrics deliver a clear message: prototype success does not guarantee production success unless industrialisation is planned and resourced from the very beginning. However, by requiring such planning, does it stifle creativity and discourage those who would have built a prototype on a hunch from doing so? I’m honestly not sure.

## The “Other 90 Percent”

So, where does this effort go? The "other 90 percent" can be thought of as the confluence of five critical, overlapping domains: Data Governance and Quality, Observability and Monitoring, Infrastructure and Deployment Pipelines, Security and Compliance, and Maintenance and Versioning.

First is Data Governance and Quality. Whether an AI system processes data in real-time streams or periodic batches, the quality and consistency of that input data are paramount. Without schema validation, lineage tracking, and automated quality checks, pipelines break. For a streaming system, this might happen instantly; for a batch system, it could cause a silent failure during an overnight job that isn't discovered for hours. To prevent this, best practices mandate implementing data versioning with tools like [Delta Lake](https://delta.io/) or [DVC](https://dvc.org/), automating data validation with frameworks like [Great Expectations](https://greatexpectations.io/), and defining clear ownership for data-quality incidents. The stakes are universal; research has demonstrated that when data quality controls are absent, deployed ML systems can suffer accuracy losses of up to 30 percent [16].

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/data_qual_gov.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
A handy analogy to understand the purpose of, and difference between, data quality and governance. Source: Experian (2017).
</div>

Second, a system must have Observability and Monitoring. Once deployed, any model's behaviour can degrade silently due to data drift, infrastructure failures, or adversarial inputs. For an online service, this might manifest as rising latency or error rates. For a batch system, it could be a gradual decline in the accuracy of its periodic outputs or an unexplained increase in processing time. Without a systematic way to track key performance metric, such as latency for services or completion time for batch jobs, along with error rates and output distributions, teams remain blind until downstream business processes are impacted. This is where observability provides deep insights into a system's health, using tools and practices to track metrics, log results, and alert on anomalies. Industry leaders like Netflix treat this deep visibility as indispensable for uptime and performance, a principle that applies to any critical AI workload [17].

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/monitor_obs.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Monitoring is concerned with viewing predefined metrics whilst observability is concerned with figuring out the internal state from a variety of external behaviours. Source: Monte Carlo (2025).
</div>

Third is the domain of Infrastructure and Deployment Pipelines. A prototype running in a data scientist's notebook is far from a scalable, resilient, and automated production workload. Production demands containerisation with tools like [Docker](https://www.docker.com/) and orchestration via platforms suited for the specific task - whether real-time services on [Kubernetes](https://kubernetes.io/), event-driven functions, or scheduled jobs on workflow orchestrators. This is all automated through continuous integration / continuous deployment (CI/CD) pipelines using systems like [GitHub Actions](https://github.com/features/actions) or [Jenkins](https://www.jenkins.io/). This task is not to be underestimated. Forrester estimates that robust CI/CD and infrastructure automation can add roughly 30 percent more development time on top of the model development itself [18]. Using Infrastructure-as-Code (IaC) to codify environments and performing scale tests with production-volume data are essential, universal practices.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/ci_cd.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
A typical CI/CD pipeline for incremental delivery of software. Source: Xenonstack (2024).
</div>

Fourth, and critically, are Security and Compliance. Production AI systems, regardless of their architecture, often process sensitive, regulated, or at least highly-valuable data, making them potential targets. A security breach can lead to devastating data loss, while violations of licensing or data-residency laws lead to legal and financial penalties. Hardening a system requires adherence to established security frameworks, such as applying the principles of the [OWASP Top Ten](https://owasp.org/www-project-top-ten/) to any web-facing components [19] and implementing robust controls like those outlined in [NIST SP 800-53](https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final) [20] across the entire system. This includes encrypting data both at rest and in transit, managing access control tightly, rotating keys, and maintaining thorough audit logs for every action the system makes. Despite this, a 2024 survey by the Cloud Security Alliance revealed that 62 percent of AI projects lacked a formal security assessment before deployment [21].

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/comp_sec.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Compliance is reactive to ensure predefined standards and regulations are met whereas security is proactive to continuously protect assets from threats. Source: Rui Pedrosa, Medium (2021).
</div>

Finally, there is Maintenance and Versioning. An AI system is never "done." Models naturally drift as the statistical properties of the data change over time (think, Zillow Offers and the housing market cool-off), and their software dependencies require constant updates. Without a disciplined maintenance strategy, which includes versioning not just the model code, but the model artifact and the data it was trained on every update risks introducing regressions and makes reproducing past results impossible. A mature approach requires using model registries like [MLflow](https://mlflow.org/) or {Seldon Hub](https://www.seldon.io/) and implementing safe validation strategies before a full rollout. For online systems, this often means blue-green or canary deployments; for batch systems, it could involve shadow deployments (running the new model in parallel with the old) or processing a small slice of data to compare outputs. Airbnb pioneered this principle of incremental validation, using canary testing to limit the impact of updates, a strategy that can be adapted for any system type [22].

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/vers.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Basic syntax of semantic versioning - a reminder that a system is never “done” whilst it still exists. You are either doing major changes, minor changes, or fixing bugs. Source: Ashu Rana, Medium (2019).
</div>

## Shell’s Global Predictive Maintenance AI

Shell is a multinational energy company that, in March 2022, deployed AI-driven predictive maintenance to over 10,000 pieces of critical equipment (e.g., pumps, compressors, valves) across its upstream, downstream and integrated-gas operations [23]. From day one, Shell embedded enterprise discipline by standardising on the C3 AI Suite on Microsoft Azure which ingests 20 billion rows weekly from more than 3 million sensors via its Delta Lake–inspired Sensor Intelligence Platform [23]. This foundation required repeatable validation. For every production model, Shell trained and tested three to four candidate variants before deployment.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/blogs/indus_90_90_rule/shell.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
Lots of different components have to work well in sync. Source: Shell.
</div>

Their deployment model focused on seamless integration into existing maintenance workflows and dashboards. Alerts and anomaly insights appear directly to technicians without disrupting protocols. Additionally, this system was commercialised industry-wide through the Open AI Energy Initiative [24]. Critically, over 10,000 production-grade ML models now generate 15 million predictions per day, with automated alerts on metric deviations and 24/7 availability backed by Azure’s multi-region failover. By making scalability, observability and resilience core features, Shell turned a pilot into a global, mission-critical service that delivers substantial cost avoidance, safety and environmental benefits.

## Conclusion

The “90/90 Rule” is a warning that is beginning to ring true for a lot of organisations getting their hands dirty with AI experimentation and industrialisation. The prototype is only half the journey. Industrialisation (covering complexities of data governance, observability, infrastructure automation, security, and maintenance) often demands resources and attention equal to or exceeding those spent on the initial model. The contrasting stories of Zillow’s Zestimate debacle and Shell’s predictive maintenance feat illustrate the two possible outcomes of this journey. A prototype is a promise, not a product. By baking in industrialisation considerations from day one, you transform that promise into a sustained, scalable, and resilient value stream.





## References

1. Garrison, T. "Inside Zillow’s Advertising Empire." Bloomberg Businessweek, 15 May 2019.

2. DelPrete, M. "The 2021 iBuyer Report." Mike DelPrete's Real Estate Tech Insights, 12 Nov 2021.

3. Henry, K. “Zillow’s U-Turn: Lessons from a $304 M Writedown.” The Wall Street Journal, 23 Jan 2022.

4. iBuyer.com. (n.d.). Zillow iBuyer Program Details. (General reference for iBuyer program launch and goals).

5. Zillow Group. (2021, November 2). Zillow Group Reports Third-Quarter 2021 Financial Results. [This is the core announcement of the Zillow Offers shutdown and the initial write-down. The $304M write-down and 25% staff layoff are referenced in the provided user text and confirmed by subsequent Zillow earnings reports.]

6. Business Wire. (2025, May 29). The U.S. Housing Market Has Nearly 500,000 More Sellers Than Buyers—the Most on Record. That Will Likely Cause Home Prices to Fall. (Provides context on housing market trends in 2022-2025).

7. Office for National Statistics. (2023, January 9). How increases in housing costs impact households. (Context on rising interest rates and their impact on housing).

8. Zillow Group. (2021, February 10). Zillow Group Q4 2020 Earnings Call.

9. Zillow Group. (2021, May 4). Zillow Group Q1 2021 Earnings Call.

10. Zillow Group. (2022, February 10). Zillow Group Reports Fourth-Quarter and Full-Year 2021 Financial Results.

12. [Historical Stock Data] Google Finance

12. Bentley, J. “Programming Pearls.” Communications of the ACM, July 1986.

13. Gartner (July 2024). “Gartner Predicts 30 % of Generative AI Projects Abandoned Post-POC.” Gartner Research (subscription).

14. Wilkinson, L. “AI Project Failure Rates Surge.” S&P Global Market Intelligence, 14 Mar 2025.

15. Gartner (2023). “AI Pilot to Production Success Rates.” Gartner Survey.

16. Budach, L. et al. “The Effects of Data Quality on Machine Learning Performance.” arXiv, 2022.

17. “Observability at Netflix.” Netflix Tech Blog (accessed June 2025).

18. Forrester Consulting (2024). “Total Economic Impact™ of AI Model Deployment Automation.”

19. OWASP Foundation. “OWASP Top Ten.” (accessed 24 Jun 2025).

20. NIST. “SP 800-53 Rev. 5: Security and Privacy Controls.” (2020).

21. Cloud Security Alliance (2024). “AI Security and Risk Survey.”

22. Airbnb Engineering & Data Science. “Continuous Delivery for Machine Learning at Airbnb.”

23. C3 (2022). “How Shell Scaled AI Predictive Maintenance to Monitor 10,000 Pieces of Equipment Globally.”

24. Journal of Petroleum Technology (2022). “Shell Scales Predictive Maintenance to 10,000 Pieces.”